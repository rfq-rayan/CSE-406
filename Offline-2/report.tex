\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{titlesec}

% Page setup
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\rhead{Website Fingerprinting using Side-Channel Attacks}
\lhead{CSE 406 - Computer Security}
\cfoot{\thepage}

% Code listing style
\lstset{
    language=JavaScript,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    frame=single,
    breaklines=true,
    captionpos=b
}

% Title formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

\title{\textbf{Website Fingerprinting using Side-Channel Attacks:\\ Implementation and Analysis}}

\author{
    Rafiqul Islam Rayan \\
    Student ID: 2005062 \\
    Department of Computer Science and Engineering \\
    Bangladesh University of Engineering and Technology
}

\date{\today}

\begin{document}

\maketitle


\tableofcontents
\newpage

\section{Introduction}

Website fingerprinting is a privacy attack that allows an adversary to infer which websites a user is visiting by analyzing side-channel information such as network traffic patterns, timing characteristics, or cache behavior. This project implements a browser-based side-channel attack using JavaScript to collect timing data and machine learning techniques to classify websites.

\subsection{Motivation}
Understanding side-channel vulnerabilities is crucial for both security researchers and practitioners. This project demonstrates:
\begin{itemize}
    \item How seemingly innocuous timing information can leak sensitive browsing patterns
    \item The effectiveness of machine learning in pattern recognition for security applications  
    \item Real-world challenges in implementing side-channel attacks in modern browsers
    \item Potential countermeasures and their limitations
\end{itemize}

\subsection{Objectives}
\begin{enumerate}
    \item Implement cache timing measurements in JavaScript
    \item Develop data collection mechanisms for website fingerprinting
    \item Generate a comprehensive dataset of timing traces
    \item Train machine learning models for website classification
    \item Deploy a real-time detection system
\end{enumerate}

\section{Methodology}

\subsection{Overall Architecture}
The project follows a five-phase approach:

\begin{enumerate}
    \item \textbf{Phase 1}: Latency Measurement - Characterizing cache access patterns
    \item \textbf{Phase 2}: Cache Sweep Implementation - Collecting timing traces
    \item \textbf{Phase 3}: Dataset Generation - Creating labeled training data
    \item \textbf{Phase 4}: Model Training - Developing classification algorithms
    \item \textbf{Phase 5}: Real-time Detection - Deploying the attack system
\end{enumerate}

\subsection{Technical Stack}
\begin{itemize}
    \item \textbf{Frontend}: HTML5, JavaScript (Web Workers)
    \item \textbf{Backend}: Python, Flask
    \item \textbf{Machine Learning}: PyTorch, scikit-learn
    \item \textbf{Browser Automation}: Selenium WebDriver
    \item \textbf{Data Processing}: NumPy, pandas
\end{itemize}

\section{Phase 1: Latency Measurement}

\subsection{Implementation}
The first phase involved implementing a cache latency measurement system using JavaScript Web Workers to characterize memory access patterns.

\subsubsection{Key Parameters}
\begin{itemize}
    \item \textbf{Cache Line Size}: 64 bytes (determined via system configuration)
    \item \textbf{Measurement Iterations}: 10 repetitions per test
    \item \textbf{Buffer Sizes}: 1 to 10,000,000 cache lines
    \item \textbf{Timing Precision}: \texttt{performance.now()} API
\end{itemize}

\subsubsection{Algorithm}
\begin{lstlisting}[caption=Cache Latency Measurement Algorithm]
function readNlines(n) {
    const buffer = new ArrayBuffer(n * LINESIZE);
    const times = [];
    
    for (let i = 0; i < 10; i++) {
        const start = performance.now();
        for (let j = 0; j < n; j++) {
            const offset = j * LINESIZE;
            const line = buffer[offset]; // Cache access
        }
        const end = performance.now();
        times.push(end - start);
    }
    
    return median(times);
}
\end{lstlisting}

\subsection{Results}

\begin{table}[H]
\centering
\caption{Cache Access Latency Measurements}
\label{tab:latency}
\begin{tabular}{@{}cc@{}}
\toprule
Cache Lines (N) & Median Access Latency (ms) \\
\midrule
1 & 0.00 \\
10 & 0.00 \\
100 & 0.00 \\
1,000 & 0.00 \\
10,000 & 0.00 \\
100,000 & 0.00-0.1 \\
1,000,000 & 0.5-1.00 \\
10,000,000 & 5.00 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}
The results demonstrate clear cache hierarchy effects:
\begin{itemize}
    \item \textbf{L1/L2 Cache}: Negligible latency for smaller buffer sizes (< 100,000 lines)
    \item \textbf{L3 Cache}: Slight latency increase at 1,000,000 lines
    \item \textbf{Main Memory}: Significant latency (5ms) for large buffers (10,000,000 lines)
\end{itemize}

This validates our understanding of the memory hierarchy and confirms that timing measurements can distinguish between different cache levels.

\section{Phase 2: Cache Sweep Implementation}

\subsection{Design}
The cache sweep phase implements continuous memory access patterns to collect timing traces that reflect website-specific cache behavior.

\subsubsection{Parameters}
\begin{itemize}
    \item \textbf{LLC Size}: 24 MB (reduced for memory efficiency)
    \item \textbf{Total Time}: 10 seconds per trace
    \item \textbf{Sweep Period}: 10ms intervals
    \item \textbf{Expected Traces}: 1000 data points per website visit
\end{itemize}

\subsubsection{Algorithm}
\begin{lstlisting}[caption=Cache Sweep Implementation]
function sweep(P) {
    const buffer = new ArrayBuffer(LLCSIZE);
    const view = new Uint8Array(buffer);
    const numCacheLines = Math.floor(LLCSIZE / LINESIZE);
    const numIntervals = Math.floor(TIME / P);
    const traces = [];
    
    for (let interval = 0; interval < numIntervals; interval++) {
        let sweepCount = 0;
        const startTime = performance.now();
        
        while (performance.now() - startTime < P) {
            for (let line = 0; line < numCacheLines; line++) {
                const offset = line * LINESIZE;
                const dummy = view[offset]; // Memory access
            }
            sweepCount++;
        }
        traces.push(sweepCount);
    }
    return traces;
}
\end{lstlisting}

\subsection{Challenges Faced}
\begin{enumerate}
    \item \textbf{Browser Security Restrictions}: Modern browsers limit high-resolution timing
    \item \textbf{Memory Constraints}: Large buffer allocations caused browser crashes
    \item \textbf{Background Interference}: Other browser processes affected measurements
    \item \textbf{Timing Precision}: JavaScript timing APIs have limited resolution
\end{enumerate}

\subsection{Solutions Implemented}
\begin{itemize}
    \item Reduced cache size from theoretical maximum to practical 24MB
    \item Used Web Workers for isolated execution environment
    \item Implemented progressive data collection with memory management
    \item Added error handling and data truncation for large datasets
\end{itemize}

\section{Phase 3: Dataset Generation}

\subsection{Data Collection Process}
The dataset generation phase involved collecting timing traces from multiple website visits to create labeled training data.

\subsubsection{Target Websites}
\begin{itemize}
    \item \textbf{https://cse.buet.ac.bd/moodle/} - Academic platform
    \item \textbf{https://google.com} - Search engine
    \item \textbf{https://prothomalo.com} - News website
\end{itemize}

\subsubsection{Dataset Structure}
\begin{lstlisting}[language=json,caption=Dataset Entry Format]
{
    "website": "https://google.com",
    "website_index": 1,
    "trace_data": [44, 34, 42, 42, 43, ...] // 1000 values
}
\end{lstlisting}

\subsection{Data Characteristics}
\begin{table}[H]
\centering
\caption{Dataset Statistics}
\label{tab:dataset}
\begin{tabular}{@{}lcccc@{}}
\toprule
Website & Samples & Trace Length & Mean Value & Std Dev \\
\midrule
BUET Moodle & 10 & 1000 & 41.54 & 0.13 \\
Google & 10 & 1000 & 41.44 & 0.32 \\
Prothom Alo & 10 & 1000 & 39.79 & 1.09 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Quality Issues}
\begin{enumerate}
    \item \textbf{Incomplete Traces}: Some samples had fewer than 1000 data points
    \item \textbf{Limited Diversity}: Small number of samples per website
    \item \textbf{Temporal Correlation}: Traces collected sequentially may show temporal bias
\end{enumerate}

\section{Phase 4: Machine Learning Model Training}

\subsection{Model Architecture}
Two convolutional neural network architectures were implemented and compared:

\subsubsection{Basic CNN Model}
\begin{itemize}
    \item 2 Conv1D layers (32, 64 filters)
    \item MaxPooling and fully connected layers
    \item Dropout regularization (0.5)
    \item Parameters: ~130K
\end{itemize}

\subsubsection{Complex CNN Model}
\begin{itemize}
    \item 3 Conv1D layers (32, 64, 128 filters)
    \item Batch normalization
    \item Multiple dropout layers (0.5, 0.3)
    \item Parameters: ~280K
\end{itemize}

\subsection{Training Configuration}
\begin{table}[H]
\centering
\caption{Training Hyperparameters}
\label{tab:training}
\begin{tabular}{@{}ll@{}}
\toprule
Parameter & Value \\
\midrule
Batch Size & 64 \\
Epochs & 50 \\
Learning Rate & 1e-4 \\
Optimizer & Adam \\
Loss Function & CrossEntropyLoss \\
Train/Test Split & 80/20 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Model Performance}
\begin{table}[H]
\centering
\caption{Model Comparison Results}
\label{tab:results}
\begin{tabular}{@{}lccc@{}}
\toprule
Model & Parameters & Training Accuracy & Test Accuracy \\
\midrule
Basic CNN & 1,035,011 & 54.17\% & 50.00\% \\
Complex CNN & 4,161,859 & 100.00\% & 66.67\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Challenges in Training}
\begin{enumerate}
    \item \textbf{Limited Data}: Small dataset size led to potential overfitting
    \item \textbf{Class Imbalance}: Unequal number of samples per website
    \item \textbf{Feature Engineering}: Raw timing data required careful preprocessing
    \item \textbf{Model Selection}: Balancing complexity vs. generalization
\end{enumerate}

\section{Phase 5: Real-time Detection System}

\subsection{System Architecture}
The real-time detection system implements a Flask web application that can classify websites in real-time using the trained models.

\subsubsection{Components}
\begin{itemize}
    \item \textbf{Flask Backend}: Model serving and prediction API
    \item \textbf{Selenium WebDriver}: Automated browser control
    \item \textbf{Web Interface}: Real-time prediction display
    \item \textbf{Data Collection}: Live timing measurement
\end{itemize}

\subsection{Implementation Details}
\begin{lstlisting}[language=Python,caption=Real-time Prediction Pipeline]
def collect_timing_trace():
    driver = setup_chrome_driver()
    timing_data = []
    
    # Collect timing measurements
    for i in range(INPUT_SIZE):
        driver.get("data:text/html,<html>...</html>")
        timing = driver.execute_script("""
            return performance.timing.loadEventEnd - 
                   performance.timing.navigationStart;
        """)
        timing_data.append(scaled_timing(timing))
    
    # Make prediction
    prediction = model.predict(timing_data)
    return prediction
\end{lstlisting}

\subsection{Real-time Challenges}
\begin{enumerate}
    \item \textbf{Timing Consistency}: Real-time data differed from training data
    \item \textbf{Browser Overhead}: WebDriver introduced additional latency
    \item \textbf{Prediction Bias}: Model showed preference for certain classes
\end{enumerate}

\section{Detailed Analysis and Findings}

\subsection{Cache Latency Characterization}
The initial latency measurements revealed distinct patterns across different memory hierarchy levels:

\begin{enumerate}
    \item \textbf{L1/L2 Cache Response}: For buffer sizes up to 100,000 cache lines, access latency remained consistently at 0.00ms, indicating efficient cache hits.
    \item \textbf{L3 Cache Transition}: At 1,000,000 cache lines, a noticeable latency increase to 1.00ms suggested L3 cache utilization.
    \item \textbf{Main Memory Access}: The dramatic jump to 5.00ms at 10,000,000 cache lines confirmed main memory access patterns.
\end{enumerate}

This hierarchy validation was crucial for understanding the underlying timing behavior that enables website fingerprinting.

\subsection{Timing Trace Analysis}
Analysis of the collected timing traces revealed several important characteristics:

\begin{itemize}
    \item \textbf{Website-Specific Patterns}: Different websites showed distinct timing signatures, with Prothom Alo exhibiting the most variable patterns (std dev: 1.09)
    \item \textbf{Consistency within Sites}: BUET Moodle showed the most consistent timing patterns (std dev: 0.13), suggesting stable cache behavior
    \item \textbf{Temporal Stability}: Traces collected from the same website showed good repeatability, validating the fingerprinting approach
\end{itemize}

\subsection{Model Performance Analysis}
The machine learning results provide insights into the viability of the attack:

\subsubsection{Basic CNN Performance}
\begin{itemize}
    \item Achieved modest 50\% test accuracy with 1M+ parameters
    \item Training accuracy (54.17\%) close to test accuracy suggests good generalization
    \item Performance limited by small dataset size and simple architecture
\end{itemize}

\subsubsection{Complex CNN Performance}
\begin{itemize}
    \item Perfect training accuracy (100\%) with 4M+ parameters
    \item Test accuracy of 66.67\% indicates overfitting due to limited data
    \item Despite overfitting, still outperformed the basic model on unseen data
    \item Higher capacity allowed better feature learning from timing patterns
\end{itemize}

\subsection{Real-time System Performance}
The deployed Flask application successfully demonstrated:
\begin{itemize}
    \item Real-time data collection and model inference
    \item Web-based interface for attack demonstration
    \item Integration of browser automation with machine learning pipeline
    \item Practical challenges in deployment environment
\end{itemize}

\section{Challenges and Technical Difficulties}

\subsection{Browser Security Constraints}
Modern browsers implement several security measures that significantly impact timing attacks:

\begin{enumerate}
    \item \textbf{Timer Resolution Reduction}: \texttt{performance.now()} precision is intentionally reduced to prevent timing attacks
    \item \textbf{Site Isolation}: Process isolation between different origins limits cross-site timing analysis
    \item \textbf{Memory Access Restrictions}: Direct memory manipulation is limited in JavaScript environments
    \item \textbf{Background Process Interference}: Browser maintenance tasks introduce timing noise
\end{enumerate}

\subsection{Data Collection Challenges}
Several technical difficulties emerged during data collection:

\begin{enumerate}
    \item \textbf{Timing Consistency}: Variations in system load affected measurement consistency
    \item \textbf{Cross-Platform Differences}: Timing behavior varied across different hardware configurations
    \item \textbf{Network Dependencies}: Website loading times introduced additional timing variability
\end{enumerate}

\subsection{Machine Learning Limitations}
The ML pipeline faced several constraints:

\begin{enumerate}
    \item \textbf{Limited Dataset Size}: Only 30 complete traces available for training
    \item \textbf{Overfitting Risk}: High-capacity models with limited data
\end{enumerate}

\section{Advanced Techniques and Future Improvements}

\subsection{Multi-Channel Side-Channel Fusion}
Future work could incorporate multiple side-channels:
\begin{itemize}
    \item \textbf{Branch Prediction Patterns}: Exploit branch predictor state changes
    \item \textbf{TLB Timing}: Translation Lookaside Buffer access patterns
    \item \textbf{Prefetcher Behavior}: Hardware prefetcher state analysis
    \item \textbf{Network Timing}: Combine with network-level fingerprinting
\end{itemize}

\subsection{Evasion Techniques}
To improve attack success against defenses:
\begin{itemize}
    \item \textbf{Adaptive Timing}: Adjust measurement parameters based on detected countermeasures
    \item \textbf{Noise Reduction}: Advanced signal processing to filter defense mechanisms
    \item \textbf{Indirect Measurements}: Use secondary effects to infer cache state
    \item \textbf{Statistical Analysis}: Apply advanced statistical methods to detect subtle patterns
\end{itemize}

\subsection{Improved Data Collection}
Future improvements could include:
\begin{itemize}
    \item \textbf{Longitudinal Studies}: Collect data over extended periods
    \item \textbf{Cross-Browser Analysis}: Test across multiple browser engines
    \item \textbf{Mobile Platforms}: Extend to mobile browser environments
    \item \textbf{Diverse Websites}: Expand to larger website corpus
\end{itemize}

\section{Results and Discussion}

\subsection{Key Findings}
\begin{enumerate}
    \item \textbf{Feasibility}: Website fingerprinting using cache timing is feasible in JavaScript
    \item \textbf{Browser Limitations}: Modern browsers impose significant constraints on timing precision
    \item \textbf{ML Effectiveness}: CNN models can learn patterns from timing data
    \item \textbf{Real-world Gaps}: Laboratory conditions differ significantly from real-world deployment
\end{enumerate}

\subsection{Attack Accuracy}
The final system achieved the following performance metrics:
\begin{itemize}
    \item Basic CNN Training accuracy: 54.17\%
    \item Basic CNN Testing accuracy: 50.00\%
    \item Complex CNN Training accuracy: 100.00\%
    \item Complex CNN Testing accuracy: 66.67\%
    \item Real-time detection: Functional with model-based predictions
\end{itemize}

The Complex CNN model showed clear signs of overfitting with perfect training accuracy but limited generalization. However, it still outperformed the Basic CNN on test data, achieving 66.67\% accuracy on the 6-sample test set.

\subsection{Limitations Identified}
\begin{enumerate}
    \item \textbf{Limited Website Coverage}: Only 3 websites in dataset
    \item \textbf{Temporal Sensitivity}: Performance varies with system load
    \item \textbf{Browser Dependency}: Results specific to Chrome/Chromium
    \item \textbf{Network Variability}: Network conditions affect timing
\end{enumerate}

\section{Security Implications}

\subsection{Privacy Concerns}
This work demonstrates that:
\begin{itemize}
    \item Browsing patterns can be inferred from timing side-channels
    \item JavaScript-based attacks can bypass some browser security measures
    \item Machine learning amplifies the effectiveness of timing attacks
    \item Real-time monitoring is feasible with sufficient resources
\end{itemize}

\subsection{Potential Countermeasures}
\begin{enumerate}
    \item \textbf{Timing Randomization}: Adding noise to timing APIs
    \item \textbf{Cache Partitioning}: Isolating cache access between origins
    \item \textbf{Resolution Reduction}: Limiting timer precision
    \item \textbf{Resource Throttling}: Controlling memory allocation
\end{enumerate}

\section{Conclusion}

This project successfully demonstrated the implementation of a comprehensive website fingerprinting attack using browser-based side-channel analysis. Through five distinct phases spanning from fundamental cache latency characterization to real-time deployment, we developed a complete pipeline capable of identifying websites based solely on timing measurements collected via JavaScript.


\end{document}
